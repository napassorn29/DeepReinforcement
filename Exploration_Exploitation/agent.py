import numpy as np
from numpy import random
import matplotlib.pyplot as plt
from bandit import MultiArmBandit
import random

class EpsilonGreedy:
    def __init__(self, epsilon=0.1, bandit = None):
        self.epsilon = epsilon
        self.action_values = np.zeros(bandit.num_bandits)
        self.action_counts = np.zeros(bandit.num_bandits)
        self.reward_history = []
        self.total_reward = 0
        self.bandit = bandit
        
    def select_action(self):
        if random.random() > self.epsilon:
            action = np.argmax(self.action_values)
        else:
            action = np.random.randint(self.bandit.num_bandits)
        return action
    
    def update(self, action, reward):
        self.action_counts[action] += 1
        self.action_values[action] += (reward - self.action_values[action]) / self.action_counts[action]
        self.reward_history.append(reward)
        self.total_reward += reward
        
    def run(self,step=100):
        for step in range(step):
            action = self.select_action()
            reward = self.bandit.pull_arm()[action]
            self.update(action, reward)
            
class UCB:
    def __init__(self, c = 2, bandit = None):
        self.c = c
        self.action_values = np.zeros(bandit.num_bandits)
        self.action_counts = np.zeros(bandit.num_bandits)
        self.reward_history = []
        self.total_pulls = 0
        self.bandit = bandit
        
    def select_action(self):
        ucb_values = self.action_values + self.c * np.sqrt(np.log(self.total_pulls + 1) / (self.action_counts + 1e-8))
        action = np.argmax(ucb_values)
        return action
    
    def update(self, action, reward):
        action = self.select_action()
        reward = self.bandit.pull_arm()[action]
        self.action_counts[action] += 1
        self.action_values[action] += (reward - self.action_values[action]) / self.action_counts[action]
        self.reward_history.append(reward)
        self.total_pulls += 1
    
    def run(self,step=100):
        for step in range(step):
            action = self.select_action()
            reward = self.bandit.pull_arm()[action]
            self.update(action, reward)
    
bandit = MultiArmBandit(3,[[-2,0,2,4],[2,3,0,4],[5,-5,10]],[[0.4,0.2,0.3,0.1],[0.2,0.1,0.5,0.2],[0.2,0.5,0.3]])
EpsilonG = EpsilonGreedy(0.8,bandit)
UCBAlgo = UCB(7,bandit)
# print(bandit.reward)
# print(bandit.prob_reward)
print(bandit.pull_arm())
EpsilonG.run(100000)
# print(EpsilonG.reward_history)
print(EpsilonG.action_values)

UCBAlgo.run(100000)
# print(UCBAlgo.reward_history)
print(UCBAlgo.action_values)