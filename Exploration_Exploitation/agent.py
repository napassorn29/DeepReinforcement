import numpy as np
from numpy import random
import matplotlib.pyplot as plt
from bandit import MultiArmBandit
import random

# Napassorn Techasombooranakit 64340500035

# set up EpsilonGreedy class
class EpsilonGreedy:
    def __init__(self, epsilon=0.1, bandit = None):
        # set up variables : epsilon, action_values, his_action_values, action_counts, reward_history, his_total_reward, total_reward, bandit
        # epsilon : exploration rate
        self.epsilon = epsilon
        # action_values : array of action values for each bandit
        self.action_values = np.zeros(bandit.num_bandits)
        # his_action_values : array of action values since the beginning for each bandit
        self.his_action_values = [[] for _ in range(bandit.num_bandits)]
        # action_counts : array of action counts for each bandit
        self.action_counts = np.zeros(bandit.num_bandits)
        # reward_history : array of reward history
        self.reward_history = []
        # his_total_reward : array of total reward since the beginning
        self.his_total_reward = []
        # total_reward : total reward
        self.total_reward = 0
        # bandit : bandit object
        self.bandit = bandit
    
    # select_action method to select action based on epsilon
    def select_action(self):
        if random.random() > self.epsilon:
            action = np.argmax(self.action_values)
        else:
            action = np.random.randint(self.bandit.num_bandits)
        return action
    
    # update method to update action values, action counts, reward history, total reward, and his_total_reward
    def update(self, action, reward):
        self.action_counts[action] += 1
        self.action_values[action] += (reward - self.action_values[action]) / self.action_counts[action]
        for i in range(self.bandit.num_bandits):
            self.his_action_values[i].append(self.action_values[i])
        self.reward_history.append(reward)
        self.total_reward += reward
        self.his_total_reward.append(self.total_reward)
    
    # run method to run the epsilon greedy algorithm
    def run(self,step=100):
        for step in range(step):
            action = self.select_action()
            reward = self.bandit.pull_arm()[action]
            self.update(action, reward)

# set up UCB class  
class UCB:
    def __init__(self, c = 2, bandit = None):
        # set up variables : c, action_values, his_action_values, action_counts, reward_history, total_pulls, his_total_reward, total_reward, bandit
        # c : exploration rate
        self.c = c
        # action_values : array of action values for each bandit
        self.action_values = np.zeros(bandit.num_bandits)
        # his_action_values : array of action values since the beginning for each bandit
        self.his_action_values = [[] for _ in range(bandit.num_bandits)]
        # action_counts : array of action counts for each bandit
        self.action_counts = np.zeros(bandit.num_bandits)
        # reward_history : array of reward history
        self.reward_history = []
        # total_pulls : total pulls
        self.total_pulls = 0
        # his_total_reward : array of total reward since the beginning
        self.his_total_reward = []
        # total_reward : total reward
        self.total_reward = 0
        # bandit : bandit object
        self.bandit = bandit
    
    # select_action method to select action based on UCB
    def select_action(self):
        ucb_values = self.action_values + self.c * np.sqrt(np.log(self.total_pulls + 1) / (self.action_counts + 1e-8))
        action = np.argmax(ucb_values)
        return action
    
    # update method to update action values, action counts, reward history, total reward, and his_total_reward
    def update(self, action, reward):
        action = self.select_action()
        reward = self.bandit.pull_arm()[action]
        self.action_counts[action] += 1
        self.action_values[action] += (reward - self.action_values[action]) / self.action_counts[action]
        for i in range(self.bandit.num_bandits):
            self.his_action_values[i].append(self.action_values[i])
        self.reward_history.append(reward)
        self.total_pulls += 1
        self.total_reward += reward
        self.his_total_reward.append(self.total_reward)
    
    # run method to run the UCB algorithm
    def run(self,step=100):
        for step in range(step):
            action = self.select_action()
            reward = self.bandit.pull_arm()[action]
            self.update(action, reward)