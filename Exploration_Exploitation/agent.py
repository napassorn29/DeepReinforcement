import numpy as np
from numpy import random
import matplotlib.pyplot as plt
from bandit import MultiArmBandit
import random

class EpsilonGreedy:
    def __init__(self, epsilon=0.1, bandit = None):
        self.epsilon = epsilon
        self.action_values = np.zeros(bandit.num_bandits)
        self.his_action_values = [[] for _ in range(bandit.num_bandits)]
        self.action_counts = np.zeros(bandit.num_bandits)
        self.reward_history = []
        self.his_total_reward = []
        self.total_reward = 0
        self.bandit = bandit
        
    def select_action(self):
        if random.random() > self.epsilon:
            action = np.argmax(self.action_values)
        else:
            action = np.random.randint(self.bandit.num_bandits)
        return action
    
    def update(self, action, reward):
        self.action_counts[action] += 1
        self.action_values[action] += (reward - self.action_values[action]) / self.action_counts[action]
        for i in range(self.bandit.num_bandits):
            self.his_action_values[i].append(self.action_values[i])
        self.reward_history.append(reward)
        self.total_reward += reward
        self.his_total_reward.append(self.total_reward)
        
    def run(self,step=100):
        for step in range(step):
            action = self.select_action()
            reward = self.bandit.pull_arm()[action]
            self.update(action, reward)
            
class UCB:
    def __init__(self, c = 2, bandit = None):
        self.c = c
        self.action_values = np.zeros(bandit.num_bandits)
        self.his_action_values = [[] for _ in range(bandit.num_bandits)]
        self.action_counts = np.zeros(bandit.num_bandits)
        self.reward_history = []
        self.total_pulls = 0
        self.his_total_reward = []
        self.total_reward = 0
        self.bandit = bandit
        
    def select_action(self):
        ucb_values = self.action_values + self.c * np.sqrt(np.log(self.total_pulls + 1) / (self.action_counts + 1e-8))
        action = np.argmax(ucb_values)
        return action
    
    def update(self, action, reward):
        action = self.select_action()
        reward = self.bandit.pull_arm()[action]
        self.action_counts[action] += 1
        self.action_values[action] += (reward - self.action_values[action]) / self.action_counts[action]
        for i in range(self.bandit.num_bandits):
            self.his_action_values[i].append(self.action_values[i])
        self.reward_history.append(reward)
        self.total_pulls += 1
        self.total_reward += reward
        self.his_total_reward.append(self.total_reward)
    
    def run(self,step=100):
        for step in range(step):
            action = self.select_action()
            reward = self.bandit.pull_arm()[action]
            self.update(action, reward)
    
# bandit = MultiArmBandit(3,[[-2,0,2,4],[2,3,0,4],[5,-5,10]],[[0.4,0.2,0.3,0.1],[0.2,0.1,0.5,0.2],[0.2,0.5,0.3]])
# # EpsilonG = EpsilonGreedy(0.5,bandit)
# # UCBAlgo = UCB(7,bandit)
# # # print(bandit.reward)
# # # print(bandit.prob_reward)
# # print(bandit.pull_arm())
# # EpsilonG.run(10000)
# # # print(EpsilonG.reward_history)
# # print(EpsilonG.action_values)
# # print(EpsilonG.his_action_values)


# UCB2 = UCB(2,bandit)
# UCB2.run(10000)
# print("epsilon :",UCB2.c)
# print("bandit :",UCB2.bandit)
# print("action_values :",UCB2.action_values)
# print("his_action_values :",UCB2.his_action_values)
# print("action_counts :",UCB2.action_counts)
# print("reward_history :",UCB2.reward_history)
# print("total_reward :",UCB2.total_reward)
# print("his_total_reward :",UCB2.his_total_reward)
# # UCBAlgo.run(100000)
# # print(UCBAlgo.reward_history)
# print(UCBAlgo.action_values)